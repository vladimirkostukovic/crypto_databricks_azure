{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24158091-00e7-4af8-a396-dcb5bc55f467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver append job completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "source = \"crypto.bronze.bybit_raw\"\n",
    "target = \"crypto.silver\"\n",
    "\n",
    "df = spark.table(source)\n",
    "\n",
    "# ========== KLINES ==========\n",
    "df_klines = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        explode(\"klines.result.list\").alias(\"k\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"ingestion_time\"),\n",
    "        col(\"k\")[0].cast(\"long\").alias(\"open_time\"),\n",
    "        col(\"k\")[1].cast(\"decimal(18,8)\").alias(\"open\"),\n",
    "        col(\"k\")[2].cast(\"decimal(18,8)\").alias(\"high\"),\n",
    "        col(\"k\")[3].cast(\"decimal(18,8)\").alias(\"low\"),\n",
    "        col(\"k\")[4].cast(\"decimal(18,8)\").alias(\"close\"),\n",
    "        col(\"k\")[5].cast(\"decimal(18,8)\").alias(\"volume\"),\n",
    "        col(\"k\")[6].cast(\"decimal(18,8)\").alias(\"quote_volume\"),\n",
    "    )\n",
    "    .withColumn(\"open_time_ts\", from_unixtime(col(\"open_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_klines.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.bybit_klines\")\n",
    "\n",
    "\n",
    "# ========== TICKER ==========\n",
    "df_ticker = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        explode(\"ticker.result.list\").alias(\"t\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"ingestion_time\"),\n",
    "        col(\"t.lastPrice\").cast(\"decimal(18,8)\").alias(\"last_price\"),\n",
    "        col(\"t.markPrice\").cast(\"decimal(18,8)\").alias(\"mark_price\"),\n",
    "        col(\"t.indexPrice\").cast(\"decimal(18,8)\").alias(\"index_price\"),\n",
    "        col(\"t.highPrice24h\").cast(\"decimal(18,8)\").alias(\"high_24h\"),\n",
    "        col(\"t.lowPrice24h\").cast(\"decimal(18,8)\").alias(\"low_24h\"),\n",
    "        col(\"t.openInterest\").cast(\"decimal(18,8)\").alias(\"open_interest\"),\n",
    "        col(\"t.fundingRate\").cast(\"decimal(18,8)\").alias(\"funding_rate\"),\n",
    "        col(\"t.bid1Price\").cast(\"decimal(18,8)\").alias(\"best_bid\"),\n",
    "        col(\"t.ask1Price\").cast(\"decimal(18,8)\").alias(\"best_ask\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_ticker.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.bybit_ticker\")\n",
    "\n",
    "\n",
    "# ========== FUNDING ==========\n",
    "df_funding = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        explode(\"funding_rate.result.list\").alias(\"fr\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"ingestion_time\"),\n",
    "        col(\"fr.fundingRate\").cast(\"decimal(18,8)\").alias(\"funding_rate\"),\n",
    "        col(\"fr.fundingRateTimestamp\").cast(\"long\").alias(\"funding_time\")\n",
    "    )\n",
    "    .withColumn(\"funding_time_ts\", from_unixtime(col(\"funding_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_funding.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.bybit_funding\")\n",
    "\n",
    "\n",
    "# ========== OPEN INTEREST ==========\n",
    "df_oi = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        explode(\"open_interest.result.list\").alias(\"oi\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"ingestion_time\"),\n",
    "        col(\"oi.openInterest\").cast(\"decimal(18,8)\").alias(\"open_interest\"),\n",
    "        col(\"oi.timestamp\").cast(\"long\").alias(\"event_time\")\n",
    "    )\n",
    "    .withColumn(\"event_time_ts\", from_unixtime(col(\"event_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_oi.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.bybit_oi\")\n",
    "\n",
    "\n",
    "# ========== ORDERBOOK ==========\n",
    "df_orderbook = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        col(\"orderbook.result.b\").alias(\"bids\"),\n",
    "        col(\"orderbook.result.a\").alias(\"asks\"),\n",
    "        col(\"orderbook.result.ts\").cast(\"long\").alias(\"event_time\"),\n",
    "        col(\"orderbook.result.u\").alias(\"update_id\"),\n",
    "        col(\"orderbook.result.seq\").alias(\"sequence\"),\n",
    "    )\n",
    "    .withColumn(\"event_time_ts\", from_unixtime(col(\"event_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_orderbook.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.bybit_orderbook\")\n",
    "\n",
    "print(\"Silver append job completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0909b9-82c4-4b62-a92e-4348150faaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binance silver append completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "source = \"crypto.bronze.binance_raw\"\n",
    "target = \"crypto.silver\"\n",
    "\n",
    "df = spark.table(source)\n",
    "\n",
    "# ========== BINANCE KLINES ==========\n",
    "df_klines = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        explode(\"klines\").alias(\"k\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"ingestion_time\"),\n",
    "        col(\"k\")[0].cast(\"long\").alias(\"open_time\"),\n",
    "        col(\"k\")[1].cast(\"decimal(18,8)\").alias(\"open\"),\n",
    "        col(\"k\")[2].cast(\"decimal(18,8)\").alias(\"high\"),\n",
    "        col(\"k\")[3].cast(\"decimal(18,8)\").alias(\"low\"),\n",
    "        col(\"k\")[4].cast(\"decimal(18,8)\").alias(\"close\"),\n",
    "        col(\"k\")[5].cast(\"decimal(18,8)\").alias(\"volume\"),\n",
    "        col(\"k\")[6].cast(\"long\").alias(\"close_time\"),\n",
    "        col(\"k\")[7].cast(\"decimal(18,8)\").alias(\"quote_volume\"),\n",
    "        col(\"k\")[8].cast(\"int\").alias(\"trades_count\")\n",
    "    )\n",
    "    .filter(\"open > 0 AND high > 0 AND low > 0 AND close > 0\")\n",
    "    .filter(\"volume >= 0 AND quote_volume >= 0\")\n",
    "    .filter(\"high >= low AND high >= open AND high >= close\")\n",
    "    .withColumn(\"open_time_ts\", from_unixtime(col(\"open_time\") / 1000).cast(\"timestamp\"))\n",
    "    .withColumn(\"close_time_ts\", from_unixtime(col(\"close_time\") / 1000).cast(\"timestamp\"))\n",
    "    .dropDuplicates([\"symbol\", \"open_time_ts\", \"close_time_ts\"])\n",
    ")\n",
    "\n",
    "df_klines.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.binance_klines\")\n",
    "\n",
    "\n",
    "# ========== BINANCE TICKER ==========\n",
    "df_ticker = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        col(\"ticker.price\").cast(\"decimal(18,8)\").alias(\"price\"),\n",
    "        col(\"ticker.time\").cast(\"long\").alias(\"event_time\")\n",
    "    )\n",
    "    .withColumn(\"event_time_ts\", from_unixtime(col(\"event_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_ticker.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.binance_ticker\")\n",
    "\n",
    "\n",
    "# ========== BINANCE FUNDING ==========\n",
    "df_funding = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        explode(\"funding_rate\").alias(\"fr\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"ingestion_time\"),\n",
    "        col(\"fr.fundingRate\").cast(\"decimal(18,8)\").alias(\"funding_rate\"),\n",
    "        col(\"fr.fundingTime\").cast(\"long\").alias(\"funding_time\"),\n",
    "        col(\"fr.markPrice\").cast(\"decimal(18,8)\").alias(\"mark_price\")\n",
    "    )\n",
    "    .withColumn(\"funding_time_ts\", from_unixtime(col(\"funding_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_funding.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.binance_funding\")\n",
    "\n",
    "\n",
    "# ========== BINANCE OPEN INTEREST ==========\n",
    "df_oi = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        col(\"open_interest.openInterest\").cast(\"decimal(18,8)\").alias(\"open_interest\"),\n",
    "        col(\"open_interest.time\").cast(\"long\").alias(\"event_time\")\n",
    "    )\n",
    "    .withColumn(\"event_time_ts\", from_unixtime(col(\"event_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_oi.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.binance_oi\")\n",
    "\n",
    "\n",
    "# ========== BINANCE ORDERBOOK ==========\n",
    "df_orderbook = (\n",
    "    df\n",
    "    .filter(\"symbol IS NOT NULL\")\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\"),\n",
    "        col(\"depth.lastUpdateId\").alias(\"last_update_id\"),\n",
    "        col(\"depth.bids\").alias(\"bids\"),\n",
    "        col(\"depth.asks\").alias(\"asks\"),\n",
    "        col(\"depth.E\").cast(\"long\").alias(\"event_time\"),\n",
    "        col(\"depth.T\").cast(\"long\").alias(\"transaction_time\")\n",
    "    )\n",
    "    .withColumn(\"event_time_ts\", from_unixtime(col(\"event_time\") / 1000).cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_orderbook.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{target}.binance_orderbook\")\n",
    "\n",
    "print(\"Binance silver append completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10bf7145-aa23-4d5b-bf9e-641c07c109a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def market_summary_for_ai():\n",
    "\n",
    "    # --------- KLINES: BINANCE ---------\n",
    "    binance_klines = (\n",
    "        spark.table(\"crypto.silver.binance_klines\")\n",
    "        .withColumn(\"exchange\", lit(\"binance\"))\n",
    "        .filter(col(\"open_time_ts\") >= current_timestamp() - expr(\"INTERVAL 1 DAY\"))\n",
    "    )\n",
    "\n",
    "    # --------- KLINES: BYBIT ---------\n",
    "    bybit_klines = (\n",
    "        spark.table(\"crypto.silver.bybit_klines\")\n",
    "        .withColumn(\"exchange\", lit(\"bybit\"))\n",
    "        .filter(col(\"open_time_ts\") >= current_timestamp() - expr(\"INTERVAL 1 DAY\"))\n",
    "    )\n",
    "\n",
    "    klines = binance_klines.unionByName(bybit_klines, allowMissingColumns=True)\n",
    "\n",
    "    klines_agg = (\n",
    "        klines.groupBy(\"symbol\")\n",
    "        .agg(\n",
    "            first(\"open\").alias(\"open\"),\n",
    "            max(\"high\").alias(\"high\"),\n",
    "            min(\"low\").alias(\"low\"),\n",
    "            last(\"close\").alias(\"close\"),\n",
    "            sum(\"volume\").alias(\"total_volume\"),\n",
    "            min(\"open_time_ts\").alias(\"period_start\"),\n",
    "            max(\"open_time_ts\").alias(\"period_end\"),\n",
    "        )\n",
    "        .filter(\"open > 0 AND high > 0 AND low > 0 AND close > 0\")\n",
    "        .filter(\"total_volume >= 0\")\n",
    "        .filter(\"high >= low AND high >= open AND high >= close\")\n",
    "    )\n",
    "\n",
    "    # --------- OI ---------\n",
    "    binance_oi = (\n",
    "        spark.table(\"crypto.silver.binance_oi\")\n",
    "        .filter(col(\"event_time_ts\") >= current_timestamp() - expr(\"INTERVAL 1 DAY\"))\n",
    "    )\n",
    "\n",
    "    bybit_oi = (\n",
    "        spark.table(\"crypto.silver.bybit_oi\")\n",
    "        .filter(col(\"event_time_ts\") >= current_timestamp() - expr(\"INTERVAL 1 DAY\"))\n",
    "    )\n",
    "\n",
    "    oi = binance_oi.unionByName(bybit_oi, allowMissingColumns=True)\n",
    "\n",
    "    oi_agg = (\n",
    "        oi.groupBy(\"symbol\")\n",
    "        .agg(\n",
    "            sum(\"open_interest\").alias(\"total_oi\"),\n",
    "            first(\"open_interest\").alias(\"first_oi\"),\n",
    "            last(\"open_interest\").alias(\"last_oi\"),\n",
    "        )\n",
    "        .withColumn(\"oi_change\", col(\"last_oi\") - col(\"first_oi\"))\n",
    "    )\n",
    "\n",
    "    # --------- TICKER ---------\n",
    "    binance_ticker = (\n",
    "        spark.table(\"crypto.silver.binance_ticker\")\n",
    "        .select(\n",
    "            \"symbol\",\n",
    "            col(\"price\").alias(\"last_price\"),\n",
    "            col(\"event_time_ts\"),\n",
    "            lit(None).cast(\"decimal(18,8)\").alias(\"spread\")\n",
    "        )\n",
    "        .filter(col(\"event_time_ts\") >= current_timestamp() - expr(\"INTERVAL 1 DAY\"))\n",
    "    )\n",
    "\n",
    "    bybit_ticker = (\n",
    "        spark.table(\"crypto.silver.bybit_ticker\")\n",
    "        .select(\n",
    "            \"symbol\",\n",
    "            \"last_price\",\n",
    "            col(\"ingestion_time\").alias(\"event_time_ts\"),\n",
    "            (col(\"best_ask\") - col(\"best_bid\")).alias(\"spread\")\n",
    "        )\n",
    "        .filter(col(\"event_time_ts\") >= current_timestamp() - expr(\"INTERVAL 1 DAY\"))\n",
    "    )\n",
    "\n",
    "    ticker = binance_ticker.unionByName(bybit_ticker, allowMissingColumns=True)\n",
    "\n",
    "    ticker_agg = (\n",
    "        ticker.groupBy(\"symbol\")\n",
    "        .agg(\n",
    "            avg(\"last_price\").alias(\"avg_price\"),\n",
    "            avg(\"spread\").alias(\"avg_spread\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------- FINAL JOIN ---------\n",
    "    final_df = (\n",
    "        klines_agg\n",
    "        .join(oi_agg, \"symbol\", \"left\")\n",
    "        .join(ticker_agg, \"symbol\", \"left\")\n",
    "        .withColumn(\"generated_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# ---- RUN + MATERIALIZE ----\n",
    "\n",
    "df = market_summary_for_ai()\n",
    "\n",
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"dbfs:/mnt/silver/market_summary_for_ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16934fb3-e60e-4cb3-8a81-c4f88bdb99d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW crypto.silver.market_summary_for_ai AS\n",
    "SELECT * FROM delta.`dbfs:/mnt/silver/market_summary_for_ai`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288e8c79-d9dd-471d-9667-a0fd1ed29c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7543663895916487>, line 8\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# CONFIG\u001B[39;00m\n",
       "\u001B[0;32m----> 8\u001B[0m OPENAI_API_KEY \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai_key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m SMART_MONEY_PROMPT \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmc_prompt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     10\u001B[0m MODEL \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/WidgetHandlerImpl.py:82\u001B[0m, in \u001B[0;36mWidgetsHandlerImpl.get\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns the current value of a widget with the given name.\u001B[39;00m\n",
       "\u001B[1;32m     44\u001B[0m \n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    :param name: Name of the argument to be accessed\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m        ```\u001B[39;00m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 82\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_notebookArguments\u001B[38;5;241m.\u001B[39mgetArgument(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_entry_point\u001B[38;5;241m.\u001B[39mgetCurrentBindings())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:304\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    306\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o441.getArgument.\n",
       ": com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named openai_key is defined\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:71)\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:257)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor731.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o441.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named openai_key is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:71)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:257)\n\tat jdk.internal.reflect.GeneratedMethodAccessor731.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o441.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named openai_key is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:71)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:257)\n\tat jdk.internal.reflect.GeneratedMethodAccessor731.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-7543663895916487>, line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# CONFIG\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m OPENAI_API_KEY \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai_key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m SMART_MONEY_PROMPT \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmc_prompt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m MODEL \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/WidgetHandlerImpl.py:82\u001B[0m, in \u001B[0;36mWidgetsHandlerImpl.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     43\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns the current value of a widget with the given name.\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    :param name: Name of the argument to be accessed\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m        ```\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 82\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_notebookArguments\u001B[38;5;241m.\u001B[39mgetArgument(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_entry_point\u001B[38;5;241m.\u001B[39mgetCurrentBindings())\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:304\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    306\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o441.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named openai_key is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:71)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:257)\n\tat jdk.internal.reflect.GeneratedMethodAccessor731.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "\n",
    "OPENAI_API_KEY = dbutils.widgets.get(\"openai_key\")\n",
    "SMART_MONEY_PROMPT = dbutils.widgets.get(\"smc_prompt\")\n",
    "MODEL = dbutils.widgets.get(\"model\")\n",
    "\n",
    "# ==============================\n",
    "# CREATE TABLE IF NOT EXISTS\n",
    "# ==============================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS crypto.silver.ai_responses (\n",
    "    sent_at STRING,\n",
    "    received_at STRING,\n",
    "    symbol STRING,\n",
    "    sentiment STRING,\n",
    "    confidence INT,\n",
    "    oi_signal STRING,\n",
    "    key_level DOUBLE,\n",
    "    summary STRING,\n",
    "    model STRING,\n",
    "    raw_response STRING\n",
    ")\n",
    "USING delta\n",
    "\"\"\")\n",
    "\n",
    "# ==============================\n",
    "# SCHEMA FOR DATAFRAME\n",
    "# ==============================\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sent_at\", StringType(), True),\n",
    "    StructField(\"received_at\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "    StructField(\"confidence\", IntegerType(), True),\n",
    "    StructField(\"oi_signal\", StringType(), True),\n",
    "    StructField(\"key_level\", DoubleType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"raw_response\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ==============================\n",
    "# OPENAI CALLER (SINGLE SYMBOL)\n",
    "# ==============================\n",
    "\n",
    "def call_openai(symbol: str, market_data: str):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SMART_MONEY_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Symbol: {symbol}\\n\\nMarket data:\\n{market_data}\"}\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.5\n",
    "    }\n",
    "\n",
    "    sent = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=60\n",
    "        )\n",
    "        recv = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return sent, recv, None, f\"ERROR:{r.status_code}\"\n",
    "\n",
    "        content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # strip markdown\n",
    "        clean = content.strip()\n",
    "        if clean.startswith(\"```json\"):\n",
    "            clean = clean[7:]\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean[3:]\n",
    "        if clean.endswith(\"```\"):\n",
    "            clean = clean[:-3]\n",
    "        clean = clean.strip()\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(clean)\n",
    "        except:\n",
    "            parsed = None\n",
    "\n",
    "        return sent, recv, parsed, content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {symbol}: {e}\")\n",
    "        return sent, datetime.now(timezone.utc).isoformat(), None, str(e)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# READ SILVER DATA\n",
    "# ==============================\n",
    "\n",
    "silver = spark.table(\"crypto.silver.market_summary_for_ai\")\n",
    "\n",
    "if silver.count() == 0:\n",
    "    raise ValueError(\"No silver data for AI analysis\")\n",
    "\n",
    "symbols = [r.symbol for r in silver.select(\"symbol\").distinct().collect()]\n",
    "print(f\"Processing {len(symbols)} symbols: {symbols}\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# PROCESS EACH SYMBOL\n",
    "# ==============================\n",
    "\n",
    "rows = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    symbol_data = silver.filter(col(\"symbol\") == symbol)\n",
    "    market_json = symbol_data.toPandas().to_json(orient=\"records\")\n",
    "    \n",
    "    sent, recv, parsed, raw = call_openai(symbol, market_json)\n",
    "    \n",
    "    # debug\n",
    "    print(f\"{symbol} raw response: {raw[:200] if raw else 'None'}...\")\n",
    "    \n",
    "    row = {\n",
    "        \"sent_at\": sent,\n",
    "        \"received_at\": recv,\n",
    "        \"symbol\": symbol,\n",
    "        \"sentiment\": None,\n",
    "        \"confidence\": None,\n",
    "        \"oi_signal\": None,\n",
    "        \"key_level\": None,\n",
    "        \"summary\": None,\n",
    "        \"model\": MODEL,\n",
    "        \"raw_response\": raw\n",
    "    }\n",
    "    \n",
    "    if parsed and isinstance(parsed, dict):\n",
    "        row[\"sentiment\"] = parsed.get(\"sentiment\")\n",
    "        row[\"confidence\"] = int(parsed.get(\"confidence\")) if parsed.get(\"confidence\") else None\n",
    "        row[\"oi_signal\"] = parsed.get(\"oi_signal\")\n",
    "        row[\"key_level\"] = float(parsed.get(\"key_level\")) if parsed.get(\"key_level\") else None\n",
    "        row[\"summary\"] = parsed.get(\"summary\")\n",
    "        print(f\"{symbol}: {row['sentiment']} (confidence: {row['confidence']})\")\n",
    "    else:\n",
    "        print(f\"{symbol}: Failed to parse\")\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# WRITE TO SILVER\n",
    "# ==============================\n",
    "\n",
    "if rows:\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "    df.write.mode(\"append\").saveAsTable(\"crypto.silver.ai_responses\")\n",
    "    print(f\"Saved {len(rows)} responses to crypto.silver.ai_responses\")\n",
    "else:\n",
    "    print(\"No responses to save\")\n",
    "\n",
    "print(\"AI analysis complete\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7543663895916486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer_batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}