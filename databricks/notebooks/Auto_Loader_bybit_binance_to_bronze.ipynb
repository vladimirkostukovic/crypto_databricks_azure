{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdba4a74-34cd-43aa-9702-031b3def1e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "\n",
    "LANDING_PATH = \"abfss://landing-dev@stcryptomedallion.dfs.core.windows.net/\"\n",
    "BRONZE_PATH = \"abfss://bronze-dev@stcryptomedallion.dfs.core.windows.net/\"\n",
    "BASE_CHECKPOINT = f\"{BRONZE_PATH}_checkpoints/\"\n",
    "BASE_TABLE = \"crypto.bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f516e76-9b10-4338-a74a-1fc2499b98c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Started stream: binance\n✓ Started stream: bybit\nSuccessfully started 2 stream(s)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, lit, array, struct, posexplode, input_file_name, regexp_extract\n",
    "\n",
    "\n",
    "# Autoloader for Binance\n",
    "def autoload_binance(name: str, path: str):\n",
    "    try:\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{BRONZE_PATH}{name}/_schema\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"multiLine\", \"true\")\n",
    "            .load(path)\n",
    "        )\n",
    "        \n",
    "        # Extract date/hour/minute\n",
    "        df_with_metadata = df.withColumn(\"_input_file\", input_file_name()) \\\n",
    "            .withColumn(\"file_date\", regexp_extract(col(\"_input_file\"), r\"date=(\\d{4}-\\d{2}-\\d{2})\", 1)) \\\n",
    "            .withColumn(\"file_hour\", regexp_extract(col(\"_input_file\"), r\"hour=(\\d{2})\", 1)) \\\n",
    "            .withColumn(\"file_minute\", regexp_extract(col(\"_input_file\"), r\"\\d{8}_\\d{2}(\\d{2})\\d{2}\\.json\", 1)) \\\n",
    "            .withColumn(\"file_second\", regexp_extract(col(\"_input_file\"), r\"\\d{8}_\\d{4}(\\d{2})\\.json\", 1))\n",
    "        \n",
    "        df_exploded = df_with_metadata.select(\n",
    "            col(\"timestamp\").alias(\"ingestion_timestamp\"),\n",
    "            col(\"file_date\"),\n",
    "            col(\"file_hour\"),\n",
    "            col(\"file_minute\"),\n",
    "            col(\"file_second\"),\n",
    "            col(\"intervals\"),\n",
    "            explode(col(\"symbols\")).alias(\"symbol_data\")\n",
    "        )\n",
    "        \n",
    "        df_symbols = df_exploded.select(\n",
    "            lit(\"binance\").alias(\"exchange\"),\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"intervals\",\n",
    "            col(\"symbol_data.symbol\").alias(\"symbol\"),\n",
    "            col(\"symbol_data.timestamp\").alias(\"symbol_timestamp\"),\n",
    "            col(\"symbol_data.intervals_included\"),\n",
    "            col(\"symbol_data.klines\").alias(\"klines\")\n",
    "        )\n",
    "        \n",
    "        df_intervals = df_symbols.select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            explode(array(\n",
    "                struct(lit(\"15m\").alias(\"interval\"), col(\"klines.15m\").alias(\"candles\")),\n",
    "                struct(lit(\"1h\").alias(\"interval\"), col(\"klines.1h\").alias(\"candles\")),\n",
    "                struct(lit(\"4h\").alias(\"interval\"), col(\"klines.4h\").alias(\"candles\")),\n",
    "                struct(lit(\"1d\").alias(\"interval\"), col(\"klines.1d\").alias(\"candles\"))\n",
    "            )).alias(\"interval_data\")\n",
    "        ).select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            col(\"interval_data.interval\").alias(\"interval\"),\n",
    "            col(\"interval_data.candles\").alias(\"candles\")\n",
    "        )\n",
    "        \n",
    "        df_candles = df_intervals.select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            \"interval\",\n",
    "            posexplode(\"candles\").alias(\"candle_index\", \"candle_array\")\n",
    "        )\n",
    "        \n",
    "        df_final = df_candles.select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            \"interval\",\n",
    "            \"candle_index\",\n",
    "            col(\"candle_array\")[0].cast(\"long\").alias(\"open_time\"),\n",
    "            col(\"candle_array\")[1].cast(\"double\").alias(\"open\"),\n",
    "            col(\"candle_array\")[2].cast(\"double\").alias(\"high\"),\n",
    "            col(\"candle_array\")[3].cast(\"double\").alias(\"low\"),\n",
    "            col(\"candle_array\")[4].cast(\"double\").alias(\"close\"),\n",
    "            col(\"candle_array\")[5].cast(\"double\").alias(\"volume\"),\n",
    "            col(\"candle_array\")[6].cast(\"long\").alias(\"close_time\"),\n",
    "            col(\"candle_array\")[7].cast(\"double\").alias(\"quote_volume\"),\n",
    "            col(\"candle_array\")[8].cast(\"int\").alias(\"trades\"),\n",
    "            col(\"candle_array\")[9].cast(\"double\").alias(\"taker_buy_base\"),\n",
    "            col(\"candle_array\")[10].cast(\"double\").alias(\"taker_buy_quote\")\n",
    "        )\n",
    "        \n",
    "        stream = (\n",
    "            df_final.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", f\"{BASE_CHECKPOINT}{name}\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(f\"{BASE_TABLE}.{name}_raw\")\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Started stream: {name}\")\n",
    "        return stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to start {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Autoloader for Bybit\n",
    "def autoload_bybit(name: str, path: str):\n",
    "    try:\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{BRONZE_PATH}{name}/_schema\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"multiLine\", \"true\")\n",
    "            .load(path)\n",
    "        )\n",
    "        \n",
    "        # Extract date/hour/minute\n",
    "        df_with_metadata = df.withColumn(\"_input_file\", input_file_name()) \\\n",
    "            .withColumn(\"file_date\", regexp_extract(col(\"_input_file\"), r\"date=(\\d{4}-\\d{2}-\\d{2})\", 1)) \\\n",
    "            .withColumn(\"file_hour\", regexp_extract(col(\"_input_file\"), r\"hour=(\\d{2})\", 1)) \\\n",
    "            .withColumn(\"file_minute\", regexp_extract(col(\"_input_file\"), r\"\\d{8}_\\d{2}(\\d{2})\\d{2}\\.json\", 1)) \\\n",
    "            .withColumn(\"file_second\", regexp_extract(col(\"_input_file\"), r\"\\d{8}_\\d{4}(\\d{2})\\.json\", 1))\n",
    "        \n",
    "        df_exploded = df_with_metadata.select(\n",
    "            col(\"timestamp\").alias(\"ingestion_timestamp\"),\n",
    "            col(\"file_date\"),\n",
    "            col(\"file_hour\"),\n",
    "            col(\"file_minute\"),\n",
    "            col(\"file_second\"),\n",
    "            col(\"intervals\"),\n",
    "            explode(col(\"symbols\")).alias(\"symbol_data\")\n",
    "        )\n",
    "        \n",
    "        df_symbols = df_exploded.select(\n",
    "            lit(\"bybit\").alias(\"exchange\"),\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"intervals\",\n",
    "            col(\"symbol_data.symbol\").alias(\"symbol\"),\n",
    "            col(\"symbol_data.timestamp\").alias(\"symbol_timestamp\"),\n",
    "            col(\"symbol_data.intervals_included\"),\n",
    "            col(\"symbol_data.klines.15m.result.list\").alias(\"klines_15m\"),\n",
    "            col(\"symbol_data.klines.1h.result.list\").alias(\"klines_1h\"),\n",
    "            col(\"symbol_data.klines.4h.result.list\").alias(\"klines_4h\"),\n",
    "            col(\"symbol_data.klines.1d.result.list\").alias(\"klines_1d\")\n",
    "        )\n",
    "        \n",
    "        df_intervals = df_symbols.select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            explode(array(\n",
    "                struct(lit(\"15m\").alias(\"interval\"), col(\"klines_15m\").alias(\"candles\")),\n",
    "                struct(lit(\"1h\").alias(\"interval\"), col(\"klines_1h\").alias(\"candles\")),\n",
    "                struct(lit(\"4h\").alias(\"interval\"), col(\"klines_4h\").alias(\"candles\")),\n",
    "                struct(lit(\"1d\").alias(\"interval\"), col(\"klines_1d\").alias(\"candles\"))\n",
    "            )).alias(\"interval_data\")\n",
    "        ).select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            col(\"interval_data.interval\").alias(\"interval\"),\n",
    "            col(\"interval_data.candles\").alias(\"candles\")\n",
    "        )\n",
    "        \n",
    "        df_candles = df_intervals.select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            \"interval\",\n",
    "            posexplode(\"candles\").alias(\"candle_index\", \"candle_array\")\n",
    "        )\n",
    "        \n",
    "        df_final = df_candles.select(\n",
    "            \"exchange\",\n",
    "            \"ingestion_timestamp\",\n",
    "            \"file_date\",\n",
    "            \"file_hour\",\n",
    "            \"file_minute\",\n",
    "            \"file_second\",\n",
    "            \"symbol\",\n",
    "            \"symbol_timestamp\",\n",
    "            \"interval\",\n",
    "            \"candle_index\",\n",
    "            col(\"candle_array\")[0].cast(\"long\").alias(\"open_time\"),\n",
    "            col(\"candle_array\")[1].cast(\"double\").alias(\"open\"),\n",
    "            col(\"candle_array\")[2].cast(\"double\").alias(\"high\"),\n",
    "            col(\"candle_array\")[3].cast(\"double\").alias(\"low\"),\n",
    "            col(\"candle_array\")[4].cast(\"double\").alias(\"close\"),\n",
    "            col(\"candle_array\")[5].cast(\"double\").alias(\"volume\"),\n",
    "            col(\"candle_array\")[6].cast(\"long\").alias(\"close_time\")\n",
    "        )\n",
    "        \n",
    "        stream = (\n",
    "            df_final.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", f\"{BASE_CHECKPOINT}{name}\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(f\"{BASE_TABLE}.{name}_raw\")\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Started stream: {name}\")\n",
    "        return stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to start {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def autoload_safe(name: str, path: str):\n",
    "    if name == \"binance\":\n",
    "        return autoload_binance(name, path)\n",
    "    elif name == \"bybit\":\n",
    "        return autoload_bybit(name, path)\n",
    "    else:\n",
    "        print(f\"Unknown exchange: {name}\")\n",
    "        return None\n",
    "\n",
    "exchanges = [\n",
    "    {\"name\": \"binance\", \"path\": f\"{LANDING_PATH}binance/\"},\n",
    "    {\"name\": \"bybit\", \"path\": f\"{LANDING_PATH}bybit/\"}\n",
    "]\n",
    "\n",
    "streams = [autoload_safe(ex[\"name\"], ex[\"path\"]) for ex in exchanges]\n",
    "active = [s for s in streams if s is not None]\n",
    "\n",
    "print(f\"Successfully started {len(active)} stream(s)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4898763147501023,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Auto_Loader_bybit_binance_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}